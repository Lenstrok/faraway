# Vio Coding Challenge

## QuickStart

### Prerequisites

Before you begin, ensure you have the following installed on your system:

- [Go](https://golang.org/doc/install) (version 1.16 or higher)
- Docker (optional, for running the database in a container)
- [golangci-lint](https://golangci-lint.run/welcome/install/) (for linters)

### Setup

1. **Run PostgreSQL (optional)**

   If you want to use Docker to run PostgreSQL, you can start it with:

    ```bash
    make run-dev-docker-compose
    ```

2. **Configure the environment**

   Edit the `env_example` file in the root dir of the project to set up your database connection details and any other configuration options.
   The database configuration is the same as the configuration in the docker compose.
   You may only edit `PARSER_FILE_PATH` (Absolute file path)

   | ENV                 | requirements            | description                                       |
   |---------------------|-------------------------|---------------------------------------------------|
   | `PARSER_FILE_PATH` | required                | absolute path for CSV file with geolocation data  |
   | `PARSER_BATCH_SIZE` | default:`5000`          | how many records will be stored in DB by one time |
   | `SERVER_PORT`       | default:`8080`          | the port on which the server will be running      |
   | `SERVER_TIMEOUT`    | default:`5s`            |                                                   |
   | `DB_USER`           | default:`admin`         |
   | `DB_PASSWORD`       | default:`admin`         |
   | `DB_NAME`           | default:`vio_challenge` |
   | `DB_HOST`           | default:`localhost`     |
   | `DB_PORT`           | default:`5432`          |

3. **Run DB Migration Component**

   ```bash
    make run-migration-app
    ```

4. **Run Parser Component**

   ```bash
    make run-parser-app
    ```
    - While the parser is running, you can start the server.  
    - In the future, the parser can be put on the cron job.
    - Also, current architecture with GeoServiceI provides a service for parsing and obtaining a Geolocation model.  
    Therefore, we can quickly connect the parser to the API if necessary.

5. **Start Server Component**

   Start REST API server to expose the geolocation data.

   ```bash
    make run-server-app
    ```

   The server will start on `http://localhost:8080` by default.  
   And also available in swagger http://localhost:8080/swagger/index.html#

6. **Optional**
  
   Run tests & linters
   ```bash
     make run-linters
     make run-unit-tests
   ```
   It would be good to add benchmarks on a real CSV file and a real database through testing.B  
   Also it's good to add End-to-end tests. We can use swagger auto client (generated by our specification) for API calls.

## CSV Parser Description

### Approach 1 (Implemented): Incremental Parsing with Batch Insertion

**Benchmarks:**

```bash
time_elapsed=43.655197416s
Alloc = 6 MiB   TotalAlloc = 2145 MiB   Sys = 20 MiB    NumGC = 687
```

**How it works**:

The CSV parser reads the file one record at a time to minimize memory usage, making it suitable for very large files.
Data is inserted into the database in batches to optimize the write operations. Deduplication is managed by DB
PostgreSQL using a primary key constraint on the `ip_address` column. To further optimization, the application could
read the file and write to the database concurrently in separate goroutines.

### Key Features:

- **Memory Efficiency**: By reading the file line-by-line, the parser avoids loading the entire file into memory.
- **Batch Insertion**: Batch insertion into the database, reducing the number of transactions and improving performance.
- **Database Deduplication**: The database's primary key constraint on `ip_address` ensures that duplicate records are
  automatically discarded. This allows for multiple CSV files to be processed without worrying about data duplication.

### Pros:

- **Low Memory Usage**: Efficient handling of large files without excessive memory consumption.
- **Simplified Deduplication**: Database deduplication reduces complexity in the application code.

### Cons:

- **Performance & limited optimization**: Relies on the database for deduplication, which can lead to performance
  bottlenecks.

### Approach 2: Preprocessing with `COPY` Command

**Benchmarks:**

```bash
time_elapsed=10.166084334s
Alloc = 160 MiB TotalAlloc = 1153 MiB   Sys = 255 MiB   NumGC = 31
```

**How it works**:

This approach utilizes PostgreSQL's `COPY` command to quickly load data into the database. Before loading, the CSV file
is processed in-memory to ensure uniqueness by using a map (set) to store unique `ip_address` values.

### Key Features:

- **High-Speed Insertion**: The `COPY` command is optimized for fast data loading in PostgreSQL.
- **In-Memory Deduplication**: Deduplication is performed before insertion by keeping track of unique IP addresses in
  memory.
- **Potential Distributed Processing**: For extremely large datasets or environments requiring horizontal scaling, the
  application could be extended to process data in a distributed manner, with each node handling its own file or a
  portion of a file. This approach would require careful management of state and consistency across nodes. And we can
  keep our set of IP addresses in some cache (e.g. Redis).

### Pros:

- **High Performance**: Significantly faster than batch insertion due to the `COPY` command.

### Cons:

- **High Memory Usage**: Storing all unique IP addresses in memory can lead to high memory consumption, which is
  impractical for very large datasets.
- **Complexity**: We must always keep a set of IP addresses, otherwise we will not be able to deduplicate records from
  new files.